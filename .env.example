## Optional: provider/API budgets removed; platform credits will cap usage.

# Default local HF models (semicolon-separated)
MODELS=meta-llama/Llama-3.1-8B-Instruct; mistralai/Mistral-7B-Instruct-v0.3; Qwen/Qwen2.5-7B-Instruct

# Optional API model for Inspect providers (off by default)
INSPECT_EVAL_MODEL=

# Runtime
DEVICE=cuda
DTYPE=bfloat16
BATCH_SIZE=4
MAX_SEQ_LEN=4096
SEEDS=123;456
 # Enable HF cloze smoke test in CI/tests (0=skip by default)
 RUN_HF_CLOZE_SMOKE=0

# Paths
LOGS_DIR=logs
RESULTS_DIR=artifacts/results
FIGS_DIR=artifacts/figs

# Hugging Face cache (set to a fast disk with enough space)
# Expect ~15–20 GB per 7B–8B model in cache (weights+tokenizer)
#HF_HOME=/mnt/ssd/.cache/huggingface
#TRANSFORMERS_CACHE=/mnt/ssd/.cache/huggingface/hub

# Consensus threshold for choices-only exploitable (majority voting)
CONSENSUS_K=2
