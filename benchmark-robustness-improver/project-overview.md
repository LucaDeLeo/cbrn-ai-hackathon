# RobustCBRN Eval ‚Äî Shortcut-Resistant Safety Benchmarks for CBRN Risks
## A Drop-in Evaluation Toolkit for Removing Multiple-Choice Shortcuts

**Project Title:** RobustCBRN Eval
**One-liner:** We ship a drop-in evaluation toolkit that **removes multiple-choice shortcuts** (and adds a *verified cloze* variant) so CBRN risk benchmarks measure *real knowledge*, not quirks of answer choices.

**Primary Track:** Track 1 - AI Model Evaluations for CBRN Risks
**Also Relevant:** Tracks 2-4 (Biosecurity, Chemical Safety, Radiological & Nuclear Monitoring)

### Executive Summary
Multiple-choice CBRN benchmarks can be "gamed" by models that exploit **answer-choice artifacts** (e.g., option frequency, lexical cues, "longest answer") rather than domain understanding. This inflates safety-relevant scores and obscures real risk. We implement a **transparent, reproducible pipeline** that **implements the ¬ßD.4 robustification procedure from Deep Ignorance** (choices-only screen + verified cloze), identifying and removing shortcut-exploitable items and evaluating models in a **choice-free cloze setup**, yielding metrics that better reflect genuine knowledge relevant to CBRN misuse.

## TL;DR ‚Äî Pre‚Äësubmission fixes

1. Correct WMDP size to **3,668 MCQs** (public set); note early comms citing 4,157 but follow paper/repo.
2. Attribute removal rate (**31.81%**) and longest‚Äëanswer (**46%**) to **Deep Ignorance ¬ßD.4**; phrase as ‚Äúwe implement ¬ßD.4‚Äù.
3. Scope to weekend: ship verified‚Äëcloze scorer + reports; demote cross‚Äëdomain and contamination checks to stretch.
4. Reduce leakage risk: publish only **salted SHA‚Äë256 IDs** and **aggregate metrics** for sensitive sets; add repo gate/pre‚Äëcommit to block CBRN text.
5. Models: justify the trio and add **pre‚Äë2024 sensitivity** model to mitigate contamination concerns.
6. Cite fresh benchmark‚Äërobustness literature (Sept 2025) and option‚Äëorder sensitivity results.

---

## üéØ Project Objectives

### Primary Goals
1. **Identify systematic biases** in existing bio/CBRN benchmarks that compromise evaluation validity
2. **Develop debiasing techniques** to create more robust safety assessments
3. **Establish evaluation quality metrics** for benchmark reliability
4. **Provide actionable recommendations** for improving current and future benchmarks

### Key Research Questions
- Do bio benchmarks exhibit answer length bias that models can exploit?
- Can models correctly answer questions without accessing the question text itself?
- What other systematic biases exist in current CBRN evaluation methodologies?
- How can we create tamper-resistant evaluations that truly measure dangerous capabilities?

### Core Deliverables (Weekend-Sized)

1. **Command-line "Robustifier"** that takes any MCQA dataset and outputs:
   - **Robust MCQA split** (removes shortcut-exploitable items via choices-only tests)
   - **Verified Cloze split** (no answer choices shown; length-normalized scoring)
   - **Audit reports**: shortcut rate, heuristic baselines, topic balance (contamination checks deferred)

2. **lm-eval harness integration** with configs for 2-3 open models

3. **Safe demo** on benign MCQA dataset (e.g., general science/math) without sensitive content

4. **Restricted-use adapters** for CBRN datasets (authorized access only; ID lists + hashing)

5. **Documentation suite**:
   - Technical report (3-5 pages)
   - Policy brief (1-2 pages)
   - Responsible release guidelines
   - Demo video (‚â§2 minutes)

---

## üìä Background & Motivation

### Current Landscape
- **WMDP**: 3,668 multiple-choice questions measuring hazardous knowledge in biosecurity, cybersecurity, and chemical security (Note: early announcements sometimes cite 4,157; the paper/repo converge on 3,668.)
- **WMDP-Robust**: Enhanced methodology removing 31.81% of questions exploitable through shortcuts
- **DeepIgnorance**: Pretraining data filtering approach to prevent dangerous capability acquisition
- **Critical Finding**: A na√Øve "pick the longest answer" rule achieves **46% accuracy** on public WMDP-Bio (per Deep Ignorance ¬ßD.4)
- **Problem**: Research shows major benchmarks (MMLU) have ~6.5% error rates and 5% performance variations from minor formatting changes
  and fresh benchmark‚Äërobustness work (Sept 2025) and option‚Äëorder sensitivity studies further underscore MCQA fragility; we align to this literature.

### Critical Gap
If safety-critical benchmarks contain similar flaws, they could:
- Falsely certify dangerous models as "safe" (31.81% of WMDP-Bio questions answerable without reading the question!)
- Fail to detect genuine biosecurity risks
- Be gamed by adversarial actors using simple heuristics
- Provide false confidence in inadequate safeguards

---

## üî¨ Methodology: Deep Ignorance ¬ßD.4 Robustification

### Core Approach: Consensus-Based Shortcut Detection

We implement and generalize the ¬ßD.4 robustification methodology from Deep Ignorance ([arXiv:2508.06601v1](https://arxiv.org/html/2508.06601v1)) to create genuinely robust benchmarks. All removal rates and heuristic baselines referenced (e.g., 31.81% removal; 46% longest‚Äëanswer) are reported in ¬ßD.4 and used here for planning/expectations.

### Phase 1: Shortcut Exploitation Detection

#### 1.1 Choices-Only Test (Primary Method)
```python
def screen_for_shortcuts(benchmark_data, models=['llama-3.3-70b', 'llama-4-scout', 'olmo-2-32b']):
    """
    Test if questions are answerable from choices alone (without the question stem)
    Per Deep Ignorance ¬ßD.4: 405/1,273 (31.81%) of WMDP-Bio failed this test
    """
    shortcut_items = []

    for item in benchmark_data:
        # Create choices-only version (remove stem, keep only A/B/C/D)
        choices_only_prompt = format_choices_only(item.choices)

        # Evaluate with diverse models (different families to avoid shared quirks)
        predictions = []
        for model in models:
            pred = evaluate_model(model, choices_only_prompt)
            predictions.append(pred == item.correct_answer)

        # Mark as shortcut if ALL models get it right (unanimous criterion)
        if all(predictions):
            shortcut_items.append(item.id)

    return shortcut_items  # In WMDP-Bio: 405 items (31.81%)
```

#### 1.2 Consensus Filter Implementation
```python
def apply_consensus_filter(benchmark_data, shortcut_items):
    """
    Remove shortcut-exploitable items to create robust version
    WMDP-Bio: 1,273 ‚Üí 868 items (31.81% removed)
    """
    robust_data = [item for item in benchmark_data if item.id not in shortcut_items]

    # Create splits for analysis
    splits = {
        'all': benchmark_data,                    # Original: 1,273 items
        'shortcuts': shortcut_items,              # Exploitable: 405 items
        'robust': robust_data,                     # Clean: 868 items
    }

    print(f"Removed {len(shortcut_items)}/{len(benchmark_data)} "
          f"({100*len(shortcut_items)/len(benchmark_data):.2f}%) exploitable items")

    return splits
```

#### 1.3 Heuristic Baseline Tests
```python
def test_simple_heuristics(benchmark_data):
    """
    Quantify gameability with trivial strategies
    Key finding: "Pick longest answer" achieves 46% on WMDP-Bio!
    """
    # Restrict to benign, domain-agnostic heuristics only
    heuristics = {
        'longest_answer': lambda choices: max(choices, key=len),
        'first_option': lambda choices: 'A',
        'last_option': lambda choices: 'D',
        'alphabetical': lambda choices: sorted(choices)[0]
    }

    results = {}
    for name, heuristic in heuristics.items():
        correct = sum(1 for item in benchmark_data
                     if heuristic(item.choices) == item.correct_answer)
        results[name] = correct / len(benchmark_data)

    return results  # WMDP-Bio: longest_answer = 46%
```

### Phase 2: Verified Cloze Variant (Eliminating MC Heuristics)

#### 2.1 Cloze Format Implementation
```python
def create_verified_cloze_split(benchmark_data):
    """
    Build cloze variant that eliminates multiple-choice heuristics entirely
    WMDP-Bio: 1,273 - 197 incompatible = 1,076 cloze questions
    """
    # Exclude items requiring all options visible
    incompatible_patterns = [
        'all of the above',
        'none of the above',
        'both a and b',
        'which of the following'
    ]

    cloze_compatible = []
    for item in benchmark_data:
        if not any(pattern in item.stem.lower() for pattern in incompatible_patterns):
            cloze_compatible.append(item)

    return cloze_compatible  # WMDP-Bio: 1,076 items
```

#### 2.2 Length-Normalized Scoring
```python
def evaluate_cloze_format(model, item):
    """
    Score each answer independently without showing other choices
    Uses length-normalized log-probability (4 separate forward passes)
    """
    scores = {}
    for choice_id, choice_text in item.choices.items():
        # Create prompt with only this answer option
        prompt = f"{item.stem}\nAnswer: {choice_text}"

        # Get length-normalized log probability
        log_prob = model.get_log_probability(prompt)
        normalized_score = log_prob / len(tokenizer.encode(choice_text))
        scores[choice_id] = normalized_score

    # Pick argmax as prediction
    return max(scores, key=scores.get)
```

Notes:
- Report both **length-normalized** scores and a **temperature-scaled** variant for sensitivity.
- Include a **permutation control**: MC permutation should affect MC accuracy but not cloze predictions.

### Phase 3: Generalized Robustification Playbook

#### 3.1 Step-by-Step Implementation for Any MCQA Benchmark
```python
class MCQARobustifier:
    """
    Drop-in robustification for any multiple-choice benchmark
    Implements the Deep Ignorance ¬ßD.4 methodology (arXiv:2508.06601v1)
    """

    def __init__(self, benchmark_name, data):
        self.name = benchmark_name
        self.data = data
        # Use diverse models with different training to avoid shared quirks
        self.reference_models = [
            'llama-3.3-70b',  # Cutoff: Dec 2023 (pre-WMDP release)
            'llama-4-scout',   # Different family (Meta)
            'olmo-2-32b'       # Different family (AllenAI)
        ]

    def robustify(self):
        """Complete robustification pipeline"""
        # A. Prep
        self.freeze_canonical_version()

        # B. Shortcut detection
        shortcuts = self.detect_shortcuts()
        print(f"Found {len(shortcuts)}/{len(self.data)} exploitable items")

        # C. Build splits
        splits = self.create_splits(shortcuts)

        # D. Report
        self.generate_report(splits)

        # E. Export
        self.export_for_lm_eval(splits)

        return splits

    def detect_shortcuts(self, criterion='unanimous'):
        """Run choices-only eval on N models"""
        exploitable = []
        for item in self.data:
            choices_only = self.format_choices_only(item)

            # Get predictions from all reference models
            correct_count = sum(
                self.eval_model(model, choices_only) == item.answer
                for model in self.reference_models
            )

            # Apply criterion (unanimous = all N models correct)
            if criterion == 'unanimous' and correct_count == len(self.reference_models):
                exploitable.append(item.id)
            elif criterion == 'majority' and correct_count > len(self.reference_models) / 2:
                exploitable.append(item.id)

        return exploitable
```

#### 3.2 Practical Knobs & Gotchas

**Critical Considerations from Deep Ignorance ¬ßD.4:**
- **Model choice matters**: Use models from different families to avoid shared training quirks (Meta + AllenAI lines)
- **Avoid contamination**: Prefer models with cutoffs predating benchmark release (include at least one strong pre‚Äë2024 baseline for sensitivity)
- **Heuristic checks**: Always run trivial baselines (longest-answer gets 46% on WMDP-Bio!)
- **Balance inspection**: After filtering, check category balance to avoid hollowing out specific subdomains
- **Cloze templating**: Some stems won't convert cleanly; exclude rather than force (WMDP excluded 197)
- **Execution realities**: Verified‚Äëcloze requires token logprobs; plan for local inference (vLLM/HF), cache logits, and use small shards for the demo.

#### 3.3 Quality Metrics & Reporting
```python
def generate_robustness_report(splits):
    """
    Report on All vs. Robust vs. Cloze to show shortcut gap
    Following ¬ßD.4 reporting structure
    """
    report = {
        'original_size': len(splits['all']),
        'shortcuts_found': len(splits['shortcuts']),
        'robust_size': len(splits['robust']),
        'cloze_size': len(splits.get('cloze', [])),
        'shortcut_percentage': 100 * len(splits['shortcuts']) / len(splits['all']),
        'heuristic_performance': test_simple_heuristics(splits['all']),
        'robust_heuristic_performance': test_simple_heuristics(splits['robust'])
    }

    # Key insight: Heuristics should perform much worse on robust split
    improvement = report['heuristic_performance']['longest_answer'] - \
                 report['robust_heuristic_performance']['longest_answer']

    print(f"Longest-answer heuristic degradation: {improvement:.2%}")

    return report
```

#### 3.4 Command-Line Interface Implementation
```python
# robustify/cli.py
import click
from pathlib import Path

@click.command()
@click.argument('dataset', type=click.Path(exists=True))
@click.option('--models', '-m', multiple=True,
              default=['llama-3.3-70b', 'llama-4-scout', 'olmo-2-32b'],
              help='Reference models for shortcut detection')
@click.option('--output', '-o', type=click.Path(), default='results/',
              help='Output directory for results')
@click.option('--safe-demo', is_flag=True,
              help='Run on benign dataset only (no CBRN content)')
@click.option('--criterion', type=click.Choice(['unanimous', 'majority']),
              default='unanimous', help='Consensus criterion for shortcuts')
def robustify(dataset, models, output, safe_demo, criterion):
    """
    RobustCBRN Eval: Remove shortcuts from MCQA benchmarks

    Example:
        robustify demo_benign.yaml --out results/benign_demo/
        robustify wmdp_bio.yaml --models llama-3.3 gpt-4 --criterion unanimous
    """

    # Safety check for CBRN content
    if not safe_demo and contains_cbrn_content(dataset):
        if not has_authorization():
            click.echo("‚ö†Ô∏è  CBRN dataset detected. Authorization required.")
            click.echo("Running in safe mode with hashed IDs only.")
            dataset = hash_sensitive_content(dataset)

    # Initialize robustifier
    robustifier = MCQARobustifier(dataset, models)

    # Run pipeline
    click.echo(f"üîç Screening {dataset} for shortcuts...")
    splits = robustifier.robustify(criterion)

    # Generate reports
    click.echo(f"üìä Generating reports in {output}...")
    report = generate_robustness_report(splits)
    save_report(report, output)

    # Summary
    click.echo(f"\n‚úÖ Robustification complete!")
    click.echo(f"   Original: {len(splits['all'])} items")
    click.echo(f"   Shortcuts: {len(splits['shortcuts'])} items ({report['shortcut_percentage']:.1f}%)")
    click.echo(f"   Robust: {len(splits['robust'])} items")
    click.echo(f"   Report: {output}/report.html")
```

---

## üõ†Ô∏è Implementation Plan

### Weekend Execution Schedule (triaged)

#### Friday (3-4 hours)
- Finalize team roles (see below)
- Pick 2-3 open models (diverse families/cutoffs)
- Implement **choices-only** generator + batch scorer
- Select a **benign** MCQA dataset and run first pass

#### Saturday AM (4 hours)
- Implement **unanimity shortcut filter** ‚Üí produce **Robust split**
- Add **verified cloze** scorer (+ length-norm)
- Build heuristic baselines (longest-answer, position, alphabetical, permutation)

#### Saturday PM (4 hours)
- Add **reports/dashboards** (HTML/Markdown)
- Run across 2-3 models; cache logs; save CSVs
- Draft **README + report** skeletons

#### Sunday (6-8 hours)
- Polish docs, graphs, and slides
- Write **policy brief** + **responsible release** notes
- Record **‚â§2-minute demo** (CLI run + report)
- Prepare submission package

### Team Roles (Minimum Viable)

1. **Eval Lead** ‚Äî Method, metrics, lm-eval integration
2. **ML Engineer** ‚Äî Batching, caching, reproducibility
3. **Safety/Governance Lead** ‚Äî Info-hazard review, policy brief
4. **PM/Presenter** ‚Äî Demo flow, slides, submission packaging

*For solo execution: Focus on choices-only test ‚Üí robust split ‚Üí basic report*

### Technical Stack
```yaml
languages:
  - Python 3.10+

libraries:
  analysis:
    - numpy, scipy: Statistical analysis
    - pandas: Data manipulation
    - scikit-learn: ML metrics and analysis

  nlp:
    - transformers: Model evaluation
    - sentence-transformers: Semantic analysis
    - nltk: Text processing

  visualization:
    - matplotlib, seaborn: Statistical plots
    - plotly: Interactive dashboards
    - streamlit: Demo application

models:
  reference_models:  # For shortcut detection
    - Llama-3.3-70B: Pre-WMDP cutoff (Dec 2023)
    - Llama-4-Scout: Different family for diversity
    - OLMo-2-32B: AllenAI model for additional diversity
    - (Sensitivity) One strong pre-2024 model (e.g., Llama-2-70B-chat or older Mistral) to assess contamination risk

  evaluation:
    - lm-eval-harness: Standardized evaluation framework
    - Custom tasks for robust/cloze variants
```

### Submission Package File Structure

```
robustcbrn-eval/
  README.md
  LICENSE (MIT)
  robustify/
    cli.py              # Main command-line interface
    choice_only.py      # Choices-only prompt generators
    cloze.py           # Verified cloze scorer (length-norm)
    heuristics.py      # Baseline tests (longest-answer, etc.)
    reports.py         # HTML/Markdown report generation
    utils.py           # Helper functions
  lm_eval_tasks/
    robust_mcqa.yaml   # Task definition for robust split
    verified_cloze.yaml # Task definition for cloze variant
  adapters/
    benign_demo/       # Safe demo on general science
    cbrn_template/     # Hashed IDs, loaders (no content)
  results/
    benign_demo/       # Demo results and visualizations
  docs/
    report.md          # Technical report (3-5 pages)
    policy_brief.md    # Policy recommendations (1-2 pages)
    responsible_release.md # Info-hazard handling
  .pre-commit-config.yaml  # Gate that blocks committing stems/choices unless SAFE_DEMO=true
  Makefile                 # Reproducible benign demo target (cache/logit flags)
  slides/
    robustcbrn.pdf     # Presentation slides
  demo.mp4             # 2-minute video demonstration
```

### Demo Flow (2 Minutes)

1. **Command-line execution** (30s):
   ```bash
   robustify demo_benign.yaml --out results/benign_demo/
   ```
   Live run on small benign dataset shard

2. **Report walkthrough** (60s):
   - Open `results/benign_demo/report.html`
   - Show **Shortcut Rate** (e.g., 31.81%)
   - Show **Choice-Only vs Robust** scores
   - Demonstrate **Longest-Answer** drops to chance on robust split
   - Show **order-sensitivity** plot (permutation control)

3. **Safety mechanisms** (20s):
   - Show how restricted adapters work (**salted SHA-256 IDs**, aggregate metrics only)
   - Explain info-hazard controls

4. **Impact summary** (10s):
   - Policy takeaways
   - Adoption pathway for labs

---

## Submission Readiness Checklist (Portal‚Äëaligned)

- Project report includes an ‚ÄúEvaluation Validity Risks‚Äù section and a ‚ÄúSecurity Considerations‚Äù appendix (leakage risks and mitigations).
- Public GitHub contains `robustify/` CLI, deterministic seeds, cached logs for the benign demo; no CBRN text.
- Pre-commit hook blocks committing stems/choices unless `SAFE_DEMO=true`.
- Makefile target reproduces the benign demo end‚Äëto‚Äëend.
- Prompts appendix includes MC, choices‚Äëonly, and verified‚Äëcloze templates; heuristic definitions included.

## Weekend Triage

Must‚Äëship (core demo):
- Choices‚Äëonly screen (unanimous or majority) across 2 models minimum (e.g., OLMo‚Äë2‚Äë32B + one Llama).
- Verified‚Äëcloze scorer with length normalization + HTML/Markdown report.
- Heuristics: longest answer, position bias, alphabetical; order‚Äësensitivity probe.
- Pre/post topic counts and KL; guardrail halts export if any topic loses >50%.
- Restricted adapters: salted hash + aggregate metrics only.

Nice‚Äëto‚Äëhave:
- lm‚Äëeval harness task defs for robust/cloze.
- Permutation‚Äëinvariance metric (Œî across 5 random orders).

Cut if time‚Äëcrunched:
- Contamination checks vs massive corpora.
- Cross‚Äëdomain runs beyond the benign demo (leave config stubs for Chem/Cyber).

## Likely Judge Questions ‚Äî Suggested Answers

1) ‚ÄúHow do we know your filter isn‚Äôt throwing away hard but fair items?‚Äù
- We report per‚Äëtopic retention, MC vs Robust score gaps, and order‚Äësensitivity before/after. We halt export if any topic loses >50% and include examples in the benign demo only.

2) ‚ÄúWhy should we trust verified‚Äëcloze?‚Äù
- It eliminates cross‚Äëoption comparisons‚Äîa known MC failure mode‚Äîby scoring options in isolation with length‚Äënormalized log‚Äëlikelihood. We also add temperature‚Äëscaled sensitivity and show MC permutation affects MC but not cloze.

3) ‚ÄúIsn‚Äôt this just Deep Ignorance‚Äôs method?‚Äù
- Yes‚Äîwe adopt ¬ßD.4 because it‚Äôs recent and defensible, then productize it as a drop‚Äëin tool with safety‚Äëaware adapters, reports, and harness integration for CBRN evaluations.

4) ‚ÄúCould releasing ‚Äòshortcut lists‚Äô help adversaries?‚Äù
- We never publish item text or plain IDs for sensitive sets‚Äîonly salted hashes and aggregate statistics, aligned with info‚Äëhazard norms.

## Risk Register

| Risk                                      | Likelihood | Impact | Mitigation |
| ----------------------------------------- | ---------: | -----: | ---------- |
| Misstated WMDP counts                     | High       | Med    | Fix counts; cite paper/repo. |
| Overclaiming novelty                      | Med        | Med    | Attribute ¬ßD.4; pitch as packaging/generalization. |
| Contamination bias in choices‚Äëonly screen | Med        | Med    | Add pre‚Äë2024 model sensitivity analysis. |
| Logprob access complicates cloze          | Med        | Med    | Use local inference (vLLM/HF); cache logits. |
| Topic hollowing after filtering           | Med        | Med    | Pre/post histograms; KL guardrail. |
| Info‚Äëhazard via per‚Äëitem outputs          | Low        | High   | Salted hashes; aggregate only; CI/pre‚Äëcommit guard. |
| Licensing breach (dataset content)        | Low        | High   | Loader pattern; redistribution checks. |
| Time/compute overrun                      | Med        | Med    | Minimal demo; shard; 2 models; cached runs. |

## Sources for Key Factual Claims

- WMDP benchmark description and 3,668 item count (paper/repo/PMLR).
- Deep Ignorance ¬ßD.4: 31.81% shortcutable items; 46% longest‚Äëanswer; verified‚Äëcloze method.
- Option‚Äëorder sensitivity in MCQA (ACL Findings 2024).
- Model availability: Llama‚Äë3.3‚Äë70B, Llama‚Äë4‚ÄëScout, OLMo‚Äë2‚Äë32B.
- Benchmark‚Äërobustness literature (Sept 2025).
- Sprint info‚Äëhazard guidance (Apart Research).

## üìà Expected Outcomes

### Deliverables
1. **Robust Benchmark Splits**:
   - Original dataset (e.g., 1,273 items for WMDP-Bio)
   - Shortcut subset (~31.81% based on WMDP findings)
   - Robust MCQA version (~868 items)
   - Verified Cloze variant (~1,076 items)

2. **Quantitative Analysis**:
   - Percentage of questions answerable without stems (target: ~30%)
   - Heuristic performance metrics (e.g., longest-answer: 46% ‚Üí <30%)
   - Model agreement statistics on choices-only tests
   - Pre/post topic histograms and **KL-divergence** guardrail; halt export if any topic loses >50%

3. **Implementation Toolkit**:
   - Automated pipeline for any MCQA benchmark
   - lm-eval tasks for robust/cloze variants
   - Index files mapping to original datasets

### Expected Findings (Per ¬ßD.4)
- **~30% of questions** likely exploitable through shortcuts
- **46% ‚Üí <30%** reduction in longest-answer heuristic success
- **High model agreement** (>90%) on shortcut questions
- **Significant performance gaps** between All vs. Robust vs. Cloze

### Generalization Targets
- Apply to WMDP-Cyber and WMDP-Chem
- Extend to MMLU science subsets
- Test on domain-specific bio benchmarks
- Create robust versions of safety-critical evaluations (via restricted adapters; no public release of sensitive stems/choices)

---

## üéØ Extension to Other CBRN Benchmarks

### Target Benchmarks for Robustification
```python
benchmarks_to_robustify = {
    'WMDP-Cyber': {
        'size': 1347,
        'expected_shortcuts': '~30%',
        'priority': 'HIGH'
    },
    'WMDP-Chem': {
        'size': 1537,
        'expected_shortcuts': '~25-35%',
        'priority': 'HIGH'
    },
    'MMLU-Biology': {
        'size': 219,
        'expected_shortcuts': '~20-30%',
        'priority': 'MEDIUM'
    },
    'MMLU-Chemistry': {
        'size': 203,
        'expected_shortcuts': '~20-30%',
        'priority': 'MEDIUM'
    },
    'Custom-Bio-Benchmarks': {
        'description': 'Any new biosecurity evaluations',
        'priority': 'CRITICAL'
    }
}
```

### Implementation Timeline
- **Hour 1-2**: Set up pipeline, load WMDP-Bio as baseline
- **Hour 3-4**: Run choices-only test on 3+ benchmarks
- **Hour 5-6**: Generate robust splits and cloze variants
- **Hour 7-8**: Comparative analysis and reporting

---

## üöÄ Innovation & Contributions

### Novel Aspects
1. **Systematic application** of the ¬ßD.4 methodology to multiple benchmarks
2. **Comparative analysis** across bio, cyber, and chem domains
3. **Automated pipeline** for benchmark robustification at scale
4. **Empirical validation** of 31.81% shortcut hypothesis across domains

### Broader Impact
- **Immediate**: Robust versions of all major CBRN benchmarks
- **Short-term**: Industry adoption of robustness testing standards
- **Long-term**: Shift from naive to robust evaluation practices
- **Field-wide**: Prevent false safety certifications

---

## ‚ö†Ô∏è Safety & Governance (Built-in)

### Information Hazard Controls
- **No dissemination of hazardous content** - code operates on dataset interfaces only
- **Two-tier release strategy**:
  - **Public**: Code, demo on benign data, synthetic examples, hashed ID lists
  - **Private/Restricted**: Evaluators run locally on sensitive sets; publish aggregate metrics only
- **Red-team review checklist**: "Does any doc reveal actionable wet-lab or synthesis steps?" ‚Üí redact/aggregate
 - Aligns with the sprint‚Äôs info‚Äëhazard guidance (Apart Research).

Additional controls for sensitive CBRN sets:
- Publish only **salted SHA-256** of item IDs; never stems/choices.
- Add a repo gate and pre-commit hook to block committing `.jsonl` with stems/choices unless `SAFE_DEMO=true`.
- CLI/config refuses to render per-item outputs for sensitive sets; public reports are aggregate only.
- Dataset loaders require local availability and license acceptance; CI guards block redistribution.

### Safety Mechanisms
- We do NOT distribute sensitive CBRN content
- Public repo contains **salted** hashed ID lists for authorized reproduction
- Responsible Release doc specifies redaction and private-sharing pathways
- Policy brief covers why shortcut-resistant evaluations reduce overclaiming

### Validation Strategy
- Test on publicly available benchmark portions only
- Validate findings across multiple datasets
- Peer review from safety researchers
- Gradual rollout of improvements

---

## üìä Evaluation Metrics to Report

### Core Metrics
- **All-MCQA accuracy** (baseline)
- **Shortcut-Only accuracy** (on flagged items) vs **Robust accuracy** (post-filter)
- **Verified-Cloze accuracy** (no multiple-choice cues)
- **Heuristic Gap**: (All-MCQA ‚àí Verified-Cloze)

### Robustness Indicators
- **Choice-Only Accuracy** (should be ~chance on robust split)
- **Longest-Answer Accuracy** (should drop from 46% to ~chance)
- **Permutation Sensitivity**: Œî accuracy across random option orders
- **Category Retention & Balance**: per-topic pre/post counts

### Optional Advanced Metrics
- **Confidence Calibration**: ECE/Brier on robust vs all
- **Contamination Score**: n-gram overlap with training data
- **Permutation-Invariance**: Œî across 5 random orders (report distribution)

---

## ‚úÖ Success Criteria (What Judges Can Verify)

1. **Transparency**: Configs, seeds, model hashes, logs included
2. **Reproducibility**: `make demo` reproduces benign run end-to-end
3. **Safety**: No sensitive content shipped; clear redaction policy
4. **Impact**: Clear evidence that shortcut removal changes risk conclusions
5. **Extensibility**: Adapters/spec for applying to chem/bio/rad/nuke datasets

---

## üìö References & Resources

### Core Papers
- **WMDP-Robust/DeepIgnorance**: [arxiv.org/html/2508.06601v1](https://arxiv.org/html/2508.06601v1) - Section D.4
- WMDP Benchmark: [arxiv.org/abs/2403.03218](https://arxiv.org/abs/2403.03218)
- DeepIgnorance: [deepignorance.ai](https://deepignorance.ai/)
- Benchmark Robustness: [arxiv.org/html/2509.04013v1](https://arxiv.org/html/2509.04013v1)
- LLM Evaluation Biases: [arxiv.org/html/2404.16966v2](https://arxiv.org/html/2404.16966v2)
 - Option Order Sensitivity (MCQA): ACL Findings 2024 ‚Äî "Large Language Models Sensitivity to the Order of Options"

### Datasets
- WMDP: [wmdp.ai](https://www.wmdp.ai/)
- HEx-PHI: [huggingface.co/datasets/LLM-Tuning-Safety/HEx-PHI](https://huggingface.co/datasets/LLM-Tuning-Safety/HEx-PHI)
- Anthropic HH-RLHF: [github.com/anthropics/hh-rlhf](https://github.com/anthropics/hh-rlhf)

### Tools & Frameworks
- **DeepIgnorance Implementation**: [github.com/EleutherAI/deep-ignorance](https://github.com/EleutherAI/deep-ignorance)
- WMDP GitHub: [github.com/centerforaisafety/wmdp](https://github.com/centerforaisafety/wmdp)
- DeepIgnorance Models: [huggingface.co/EleutherAI/deep-ignorance-pretraining-stage-weak-filter](https://huggingface.co/EleutherAI/deep-ignorance-pretraining-stage-weak-filter)
- lm-eval tasks: `lm_eval_tasks/` directory with robust/cloze variants

### Additional References
- Sprint page & info‚Äëhazard guidance: Apart Research CBRN AI Risks Research Sprint
- Model availability: Llama‚Äë3.3‚Äë70B (Vertex AI), Llama‚Äë4‚ÄëScout (Hugging Face), OLMo‚Äë2‚Äë32B (Hugging Face)

---

## üèÜ Alignment with Judging Criteria

### CBRN Relevance ‚úÖ
- Directly improves CBRN evaluation infrastructure
- Builds on WMDP and biosecurity literature
- Addresses critical gaps in safety assessment

### AI Safety Contribution ‚úÖ
- Foundational improvement to all safety evaluations
- Novel methodology for benchmark validation
- Measurable impact on evaluation reliability

### Execution Quality ‚úÖ
- Clear, achievable objectives
- Reproducible methodology
- Well-documented codebase
- Practical, immediate applications

---

## üë• Team Roles (if applicable)

### Suggested Division
- **Lead Analyst**: Bias detection and statistical analysis
- **ML Engineer**: Model evaluation and testing
- **Data Scientist**: Cross-benchmark validation
- **Visualization**: Dashboard and reporting

### Solo Execution Plan
If working alone, prioritize:
1. Answer length bias (most concrete)
2. Answerability without questions (most impactful)
3. One additional novel bias
4. Basic debiasing demonstration

---

## üìù Notes & Additional Considerations

### Future Extensions
- Extend analysis to cybersecurity and chemical benchmarks
- Develop automated benchmark quality assessment tools
- Create benchmark certification standards
- Build continuous monitoring for benchmark degradation

### Success Criteria
- Identify at least 3 significant biases in existing benchmarks
- Demonstrate >10% improvement in robustness metrics
- Provide actionable recommendations adopted by benchmark creators
- Open-source all detection and correction tools

### Contact & Collaboration
- Open to collaboration with benchmark creators
- Seeking feedback from safety researchers
- Planning to share findings with WMDP and DeepIgnorance teams
- Available for follow-up research partnerships

---

*This project implements and extends the Deep Ignorance ¬ßD.4 robustification methodology to create genuinely robust CBRN benchmarks. By removing the ~31.81% of questions that can be answered without reading the stem, and providing verified cloze alternatives, we ensure that safety evaluations actually measure dangerous knowledge rather than test-taking skills. This is critical infrastructure for AI safety ‚Äî without robust benchmarks, we cannot accurately assess or mitigate CBRN risks.*

**Key Innovation**: While ¬ßD.4 demonstrated this for bio benchmarks, we systematically apply and validate the procedure across C, B, R, and N domains, packaging it as a drop‚Äëin, safety‚Äëaware toolkit with reports and adapters.

---

## üìÑ Abstract for Submission

> **RobustCBRN Eval** is an open-source toolkit that makes CBRN risk evaluations **shortcut-resistant**. We detect and remove multiple-choice items that models answer correctly **without seeing the question** (31.81% in WMDP-Bio), and we add a **verified cloze** variant that eliminates multiple-choice cues entirely. The toolkit outputs robust splits, heuristic probes, and clear reports so labs and policymakers can trust that measured capabilities reflect **genuine knowledge** rather than artifacts. We demonstrate the pipeline on a benign dataset and provide restricted adapters (hashed IDs, no content) for sensitive CBRN sets. We also include a policy brief and responsible-release guidance. Our goal is to improve the **validity, safety, and reproducibility** of CBRN model evaluations, preventing false safety certifications that could have catastrophic consequences.
