Status: Done

Story
- As an evaluation owner,
- I want a true HF log-prob cloze path that emits Inspect-compatible logs,
- so that I can compare MCQ vs Cloze fairly and aggregate results consistently.

Acceptance Criteria
- Running a CLI path for cloze HF scoring writes Inspect-compatible JSON logs per run under `logs/` with fields `id`, `pred_index`, `confidence`, `target_index`, `correct`, `model`, `task`, and `seed`.
- The path sets `task` tag containing `cloze` and includes `hf_logprob` vs `fallback_structured` in tags or metadata to distinguish modes.
- `scripts/run_evalset.sh` supports a flag/env (e.g., `CLOZE_MODE=hf|fallback`) to switch between Inspect structured solver and HF log-prob path. Default remains fallback for portability.
- The HF path supports `--model`, `--dataset_path`, `--max_items`, `--device`, and `--dtype` and computes length-normalized log P(choice|stem). Confidence is the softmax probability of the chosen option.
- Logs include stem/choice indices only (no raw text), and do not violate release validation.
- Documentation explains when to use HF vs fallback and expected performance trade-offs.

Tasks / Subtasks
- [x] Add `robustcbrn/tasks/cloze_logprob.py` CLI wrapper that:
  - [x] loads dataset via `load_mcq_dataset`,
  - [x] runs `score_cloze_options(model, stems, choices)` (refactor out of `cloze_full` for reuse if needed),
  - [x] writes Inspect-like JSON log including `pred_index` and `confidence`.
- [x] Update `scripts/run_evalset.sh` to honor `CLOZE_MODE`:
  - [x] `hf`: call `.venv/bin/python -m robustcbrn.tasks.cloze_logprob ...`
  - [x] `fallback`: keep current `.venv/bin/inspect eval robustcbrn.tasks.cloze_full:cloze_full ...`
- [x] Ensure tags/metadata differentiate modes (`hf_logprob` vs `fallback_structured`).
- [x] Add docs to `docs/getting-started/usage.md` covering HF mode, device/dtype, and caveats.
- [x] Ensure `scripts/validate_release.sh` is satisfied by the HF logs (no raw text fields).

Dev Notes
- Reference files: `robustcbrn/tasks/cloze_full.py`, `robustcbrn/tasks/common.py`, `scripts/run_evalset.sh`.
- Model/tokenizer import: `transformers.AutoModelForCausalLM`, `AutoTokenizer`; handling pad/eos as in `cloze_full`.
- Confidence = softmax over normalized log-probs; ensure numeric stability via max-subtraction.
- Keep outputs aligned to aggregator expectations (`pred_index`, `target_index`, `correct`, `confidence`).

Testing
- Local smoke: `CLOZE_MODE=hf DATASET=data/sample_sanitized.jsonl SUBSET=32 make run` then inspect `logs/*cloze*` for fields.
- Aggregation: `make aggregate` and confirm `summary.json.tasks` includes `cloze` and counts reflect log rows.
- CI: add a tiny-model smoke test behind a flag (see separate CI story).

File List
- Added `robustcbrn/tasks/cloze_logprob.py` (HF log-prob cloze CLI; Inspect-compatible logs).
- Updated `scripts/run_evalset.sh` (honor `CLOZE_MODE=hf|fallback`; pass `DEVICE`/`DTYPE`).
- Updated `docs/getting-started/usage.md` (HF vs fallback usage and trade-offs).

Change Log
- 2025-09-14 v2 Implemented HF cloze path, runner switch, and docs (Dev)
- 2025-09-14 v1 Draft initial story (SM)

## QA Results

### Review Date: 2025-09-14

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
- Implementation meets all Acceptance Criteria for HF cloze path: correct CLI args, length-normalized log P(choice|stem), softmax confidence, Inspect-compatible envelope, tags `["cloze", "hf_logprob"]`, and no raw text in logs.
- Runner supports `CLOZE_MODE=hf|fallback` with default `fallback`; HF path honors `DEVICE`/`DTYPE`. Documentation clearly explains when to use each mode and trade-offs.
- Aggregator ingests the produced logs (verified fields supported by `robustcbrn.analysis.aggregate`). Sample smoke log present under `logs_runner_smoke/` matches the expected schema.
- Code leverages existing `score_cloze_options` and dataset loader; behavior is cohesive and minimal. No security-sensitive changes; dataset loader already includes basic path bounds checks.

### Refactoring Performed
- None. No code changes were required during QA; suggestions recorded below.

### Compliance Check
- Coding Standards: ✓
- Project Structure: ✓
- Testing Strategy: ✓ (story includes smoke guidance; CI tiny-model test tracked separately)
- All ACs Met: ✓

### Improvements Checklist
- [ ] Add a tiny-model unit test for `score_cloze_options` and CLI wiring (shape, determinism on seed; mark slow/optional).
- [ ] Include dataset fingerprint (e.g., SHA-1 of JSONL) in log envelope metadata for reproducibility.
- [ ] Consider basic batching or micro-optimizations for option scoring to improve throughput on large subsets.
- [ ] Add explicit CPU fallback note in docs (performance caveat already covered via `DEVICE`/`DTYPE`).
- [ ] Document that confidence is option-softmax over normalized log-probs for clarity in downstream analysis.

### Security Review
- No new exposed surfaces. Logs omit raw text. Dataset path validation remains conservative; acceptable for internal runs.

### Performance Considerations
- Current per-item option loop is adequate for target scales; batching could further reduce overhead when feasible.

### Files Modified During Review
- None

### Gate Status
Gate: PASS → docs/qa/gates/1.1-cloze_hf_path.yml
Risk profile: docs/qa/assessments/1.1-risk-20250914.md (not created)
NFR assessment: docs/qa/assessments/1.1-nfr-20250914.md (not created)

### Recommended Status
✓ Ready for Done
