Status: Draft

Story
- As a report consumer,
- I want polished docs with filled metrics, robustness interpretation, and model cards,
- so that I can understand results and reproduce runs confidently.

Acceptance Criteria
- `docs/REPORT.md` placeholders are replaced with real results after a full run, including: overall accuracy, choices-only exploitable %, position bias deltas, MCQ↔Cloze gap with CI, abstention/overconfidence, runtime/cost.
- A short “Robustness results interpretation” section/page is added, linking to metric definitions (consistency, flip rate, delta accuracy).
- A “Model cards used + versions” snippet is added to the report listing model ids, revisions, seeds, and key config.
- `docs/RESULTS_TEMPLATE.md` references updated figures and tables consistent with new aggregator outputs.

Tasks / Subtasks
- Add/expand a section in `docs/ROBUSTNESS_PLAYBOOK.md` or a new page describing robustness metrics and how to read them.
- Add a “Model cards used” subsection to `docs/REPORT.md` with placeholders and instructions to fill post-run; provide examples.
- After a full run on the target dataset, populate `docs/REPORT.md` with real numbers and figure references.

Dev Notes
- Ensure only aggregate-safe information is included (no raw text, no per-item exploit labels).
- Reference `artifacts/results/summary.json` and `artifacts/figs/` for filling numbers and images.

Testing
- Manual: run a small subset and verify the report can be filled mechanically from artifacts.

Change Log
- 2025-09-14 v1 Draft initial story (SM)

